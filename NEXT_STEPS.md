# üöÄ N√§sta Steg: Koppla GitHub till RunPod

## ‚úÖ Steg 1: Koppla GitHub till RunPod

1. **G√• till RunPod Dashboard**
   - https://www.runpod.io
   - Logga in

2. **√ñppna Settings**
   - Klicka p√• din profil (h√∂gst upp till h√∂ger)
   - V√§lj **"Settings"**

3. **Koppla GitHub**
   - I Settings, hitta **"Connections"** eller **"Integrations"**
   - Hitta **GitHub**-kortet
   - Klicka p√• **"Connect"** eller **"Authorize"**
   - F√∂lj instruktionerna f√∂r att auktorisera RunPod
   - **Viktigt:** V√§lj att ge RunPod √•tkomst till ditt repo `asdqwe222-d/nocturne-reply`

---

## ‚úÖ Steg 2: Skapa Serverless Endpoint fr√•n GitHub

1. **G√• till Serverless**
   - I RunPod Dashboard, klicka p√• **"Serverless"** i menyn

2. **Skapa nytt Endpoint**
   - Klicka p√• **"New Endpoint"** eller **"Create Endpoint"**

3. **V√§lj "Import Git Repository"**
   - Du b√∂r se flera alternativ: "Docker Image", "Import Git Repository", etc.
   - V√§lj **"Import Git Repository"**

4. **V√§lj ditt repository**
   - I dropdown-menyn, v√§lj: `asdqwe222-d/nocturne-reply`
   - **Branch:** `main` (eller `master` om det √§r standard)
   - **Optional:** V√§lj release/tag om du har skapat en

---

## ‚úÖ Steg 3: Konfigurera Endpoint

### Basic Settings:
- **Endpoint Name:** `nocturne-ollama` (eller valfritt namn)
- **GPU Type:** 
  - **A40** (f√∂r st√∂rre modeller, 70B+) - ~$0.00029/sekund
  - **RTX 3090** (f√∂r mindre modeller, 12B) - ~$0.00019/sekund
- **Worker Type:**
  - **Flex** (pay-per-use, rekommenderat f√∂r test)
  - **Active** (alltid p√•, 20-30% rabatt, b√§ttre f√∂r produktion)

### Build Settings:
- **Dockerfile Path:** `gpt-relay-server/Dockerfile` (eller bara `Dockerfile` om den √§r i root)
- **Handler Path:** `gpt-relay-server/handler.py` (eller bara `handler.py` om den √§r i root)

**OBS:** Om din Dockerfile och handler.py √§r i `gpt-relay-server/` mappen, m√•ste du ange s√∂kv√§gen korrekt!

### Environment Variables:
L√§gg till dessa:
```
OLLAMA_MODEL=nocturne-swe
OLLAMA_HOST=0.0.0.0:11434
```

### Advanced Settings (Optional):
- **Container Disk:** 10 GB (standard)
- **Volume:** 0 GB (beh√∂vs inte f√∂r nu)
- **Ports:** 11434/http (redan konfigurerat i Dockerfile)

---

## ‚úÖ Steg 4: Skapa och V√§nta p√• Build

1. **Klicka p√• "Create Endpoint"**
2. **V√§nta p√• build** (2-5 minuter)
   - Du kommer se build-loggar
   - Status kommer √§ndras fr√•n "Building" ‚Üí "Ready"
3. **Kopiera Endpoint URL**
   - Format: `https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run`
   - Denna URL beh√∂ver du i n√§sta steg!

---

## ‚úÖ Steg 5: Uppdatera `.env`

√ñppna `gpt-relay-server/.env` och l√§gg till:

```bash
# RunPod Serverless Configuration
RUNPOD_ENDPOINT_URL=https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run
RUNPOD_API_KEY=your-runpod-api-key-here

# Hybrid mode (rekommenderat)
USE_RUNPOD=false
RUNPOD_FALLBACK=true
```

**Var hittar jag API Key?**
1. RunPod Dashboard ‚Üí Settings ‚Üí API Keys
2. Klicka p√• "Create API Key"
3. Kopiera nyckeln direkt (den visas bara en g√•ng!)

---

## ‚úÖ Steg 6: Testa Endpoint

### 6.1 Testa fr√•n RunPod Dashboard

1. G√• till ditt endpoint i RunPod Dashboard
2. Klicka p√• **"Test"** eller **"Test Endpoint"**
3. Anv√§nd test-input:
```json
{
  "input": {
    "model": "nocturne-swe",
    "prompt": "Say hello in Swedish",
    "system": "You are a helpful assistant.",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "num_predict": 50
    }
  }
}
```

### 6.2 Testa fr√•n din server

Starta din server:
```bash
cd gpt-relay-server
npm start
```

Du b√∂r se i loggarna:
```
üåê RunPod Serverless: ENABLED (fallback mode)
   Endpoint: https://api.runpod.io/v2/...
   Will fallback to RunPod if local Ollama fails
```

---

## ‚úÖ Steg 7: Testa Full Generation

1. √ñppna din Tampermonkey script p√• en test-sida
2. Skriv ett meddelande i chatten
3. Klicka p√• "Generate" eller tryck Ctrl+Enter
4. Om lokal Ollama inte fungerar, ska den automatiskt fallback till RunPod!

---

## üîß Troubleshooting

### Problem: "Repository not found" i RunPod

**L√∂sning:**
- Kontrollera att GitHub √§r kopplat i RunPod Settings
- Verifiera att RunPod har √•tkomst till `asdqwe222-d/nocturne-reply`
- F√∂rs√∂k koppla om GitHub

### Problem: "Build failed"

**L√∂sning:**
- Kontrollera build-loggar i RunPod Dashboard
- Verifiera att Dockerfile finns i r√§tt mapp
- Kontrollera att `handler.py` finns
- Se till att `requirements.txt` finns

### Problem: "Handler not found"

**L√∂sning:**
- Kontrollera "Handler Path" i endpoint-konfigurationen
- Om filer √§r i `gpt-relay-server/`, anv√§nd: `gpt-relay-server/handler.py`
- Om filer √§r i root, anv√§nd: `handler.py`

### Problem: "Ollama model not found"

**L√∂sning:**
- Modellen `nocturne-swe` m√•ste vara installerad i Ollama
- I Dockerfile, l√§gg till steg f√∂r att ladda ner modellen:
  ```dockerfile
  RUN ollama pull nocturne-swe
  ```
- Eller √§ndra `OLLAMA_MODEL` environment variable till en modell som finns

---

## üìã Checklista

- [ ] GitHub kopplat till RunPod
- [ ] Endpoint skapad fr√•n GitHub repo
- [ ] Endpoint build klar (status: "Ready")
- [ ] Endpoint URL kopierad
- [ ] RunPod API Key skapad och kopierad
- [ ] `.env` uppdaterad med endpoint URL och API key
- [ ] Server startad och visar RunPod-konfiguration
- [ ] Test-k√∂rning genomf√∂rd

---

## üéâ Klart!

N√§r allt √§r klart kommer din server automatiskt anv√§nda RunPod om lokal Ollama inte fungerar!

**N√§sta:** Testa genom att generera n√•gra svar och se att RunPod anv√§nds n√§r lokal Ollama inte √§r tillg√§nglig.

