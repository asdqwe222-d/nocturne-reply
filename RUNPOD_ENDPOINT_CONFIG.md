# üéØ RunPod Endpoint Konfiguration Guide

## Viktigt: V√§lj r√§tt typ av endpoint

RunPod har tv√• s√§tt att skapa endpoints:

### Option 1: Import Git Repository (Rekommenderat f√∂r oss)
- V√§lj **"Import Git Repository"** tab
- Detta anv√§nder din GitHub repo med Dockerfile och handler.py

### Option 2: Template/Model (Om du ser Model-f√§ltet)
- Detta √§r f√∂r att k√∂ra modeller direkt fr√•n Hugging Face
- **Inte vad vi beh√∂ver** - vi anv√§nder Ollama ist√§llet

---

## Om du ser "Model" f√§ltet (Template-baserad)

Om du ser ett f√§lt f√∂r "Model" eller "Hugging Face", beh√∂ver du:

1. **Hitta "Import Git Repository" alternativet**
   - Leta efter tabs eller dropdown: "Template", "Docker Image", "Git Repository"
   - V√§lj **"Git Repository"** eller **"Import Git Repository"**

2. **Om du inte hittar Git-alternativet:**
   - Klicka p√• "New Endpoint" igen
   - Leta efter en dropdown eller tabs h√∂gst upp
   - V√§lj "Custom" eller "Git Repository"

---

## Om du ser Git Repository-alternativet

### Steg 1: V√§lj Repository
- Repository: `asdqwe222-d/nocturne-reply`
- Branch: `main`

### Steg 2: Container Configuration

**Container Start Command:**
```
python /app/handler.py
```

**Container Disk:**
- S√§tt till **10 GB** (eller minst 5 GB)
- Ollama och modeller tar plats

**Expose HTTP Ports:**
```
11434
```

**Expose TCP Ports:**
```
(leave empty)
```

### Steg 3: Environment Variables

Klicka f√∂r att expandera och l√§gg till:

```
OLLAMA_MODEL=nocturne-swe
OLLAMA_HOST=0.0.0.0:11434
```

### Steg 4: GPU Selection

V√§lj GPU:
- **A40** (f√∂r st√∂rre modeller, 70B+)
- **RTX 3090** (f√∂r mindre modeller, 12B)

### Steg 5: Worker Type

- **Flex** (pay-per-use, rekommenderat f√∂r test)
- **Active** (alltid p√•, 20-30% rabatt)

---

## Om du m√•ste anv√§nda Template/Model-versionen

Om RunPod bara visar "Model" f√§ltet och inte Git-alternativet, kan du:

### Alternativ A: Anv√§nd Docker Hub ist√§llet

1. Build och push till Docker Hub (se `DOCKER_HUB_SETUP.md`)
2. I RunPod, v√§lj **"Docker Image"** ist√§llet
3. Ange: `yourusername/nocturne-ollama:latest`

### Alternativ B: Anv√§nd Template med Ollama

Om du m√•ste anv√§nda Template-versionen:

1. **Model:** L√§mna tomt eller ange en Ollama-modell fr√•n Hugging Face
2. **Container Start Command:**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh && ollama serve & python /app/handler.py
   ```
3. **Container Disk:** 10 GB
4. **HTTP Ports:** 11434
5. **Environment Variables:**
   ```
   OLLAMA_MODEL=nocturne-swe
   OLLAMA_HOST=0.0.0.0:11434
   ```

**OBS:** Detta kr√§ver att du har en Docker image p√• Docker Hub eller att du konfigurerar en custom template.

---

## Rekommenderad v√§g: Docker Hub

Om Git Repository-alternativet inte finns, rekommenderar jag **Docker Hub**:

1. **Build image:**
   ```bash
   cd gpt-relay-server
   docker build -t yourusername/nocturne-ollama:latest .
   docker push yourusername/nocturne-ollama:latest
   ```

2. **I RunPod:**
   - V√§lj "Docker Image" tab
   - Ange: `yourusername/nocturne-ollama:latest`
   - Konfigurera som ovan

---

## Snabb Checklista

- [ ] Hittat "Import Git Repository" eller "Docker Image" alternativ
- [ ] Repository/Docker image vald
- [ ] Container Start Command: `python /app/handler.py`
- [ ] Container Disk: 10 GB
- [ ] HTTP Ports: 11434
- [ ] Environment Variables: `OLLAMA_MODEL=nocturne-swe`, `OLLAMA_HOST=0.0.0.0:11434`
- [ ] GPU vald (A40 eller RTX 3090)
- [ ] Worker Type vald (Flex rekommenderat)
- [ ] Klickat "Deploy Endpoint"

---

## N√§r endpoint √§r deployad

1. Kopiera Endpoint URL
2. Skapa API Key (Settings ‚Üí API Keys)
3. Uppdatera `.env`:
   ```bash
   RUNPOD_ENDPOINT_URL=https://api.runpod.io/v2/YOUR_ID/run
   RUNPOD_API_KEY=your-key-here
   RUNPOD_FALLBACK=true
   ```

---

**Beh√∂ver du hj√§lp med att hitta Git Repository-alternativet?** Kolla om det finns tabs eller en dropdown h√∂gst upp p√• sidan n√§r du skapar nytt endpoint.

