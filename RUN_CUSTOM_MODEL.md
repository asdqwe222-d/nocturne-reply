# üéØ K√∂ra Din Egen Modell (nocturne-swe) p√• RunPod

## Problem

`nocturne-swe` √§r en anpassad modell som skapas fr√•n en `Modelfile`. Den finns inte i Ollama's modell-register, s√• RunPod kan inte ladda ner den automatiskt.

---

## ‚úÖ L√∂sning: Skapa Modellen Automatiskt vid Startup

Vi kommer att:
1. **Inkludera Modelfile i Docker image**
2. **Uppdatera handler.py att skapa modellen vid startup**
3. **F√∂rst pulla basmodellen** (`mistral-nemo:12b-instruct-2407-q4_K_M`)

---

## Steg 1: Kopiera Modelfile till gpt-relay-server

```bash
# Fr√•n projektets root
copy nocturne-reply-forge\Modelfile gpt-relay-server\Modelfile
```

Eller kopiera manuellt:
- **Fr√•n:** `nocturne-reply-forge/Modelfile`
- **Till:** `gpt-relay-server/Modelfile`

---

## Steg 2: Uppdatera Dockerfile

Dockerfile beh√∂ver inkludera Modelfile:

```dockerfile
COPY Modelfile /app/Modelfile
```

---

## Steg 3: Uppdatera handler.py

Handler beh√∂ver:
1. Pulla basmodellen (`mistral-nemo:12b-instruct-2407-q4_K_M`)
2. Skapa `nocturne-swe` fr√•n Modelfile om den inte finns
3. V√§nta tills modellen √§r klar innan den accepterar requests

---

## Steg 4: Pusha till GitHub

```bash
cd gpt-relay-server
git add Modelfile Dockerfile handler.py
git commit -m "Add support for custom nocturne-swe model"
git push
```

---

## Steg 5: Uppdatera RunPod Endpoint

1. **G√• till RunPod Dashboard ‚Üí Edit Endpoint**
2. **Environment Variables:**
   ```
   OLLAMA_MODEL_NAME=nocturne-swe
   ```
3. **Spara Endpoint**
4. **V√§nta p√• att builden √§r klar** (kan ta 5-10 minuter f√∂rsta g√•ngen eftersom den laddar ner basmodellen)

---

## ‚ö†Ô∏è Viktigt: F√∂rsta K√∂rningen Tar Tid

N√§r endpoint startar f√∂rsta g√•ngen:
1. ‚úÖ Ollama startar
2. ‚úÖ Pullar `mistral-nemo:12b-instruct-2407-q4_K_M` (~7GB, tar 2-5 minuter)
3. ‚úÖ Skapar `nocturne-swe` fr√•n Modelfile (~30 sekunder)
4. ‚úÖ Modellen √§r klar att anv√§nda

**Efter f√∂rsta k√∂rningen:** Modellen √§r cachad och startup √§r snabbare.

---

## üß™ Testa

Efter att endpoint √§r "Ready":

```bash
cd gpt-relay-server
node server.js
```

Testa fr√•n `http://localhost:3000/test-chat.html`

---

## üìã Alternativ: Anv√§nd Standardmodell F√∂rst

Om du vill testa snabbt f√∂rst:

1. **Anv√§nd `llama3` eller `mistral`** (finns automatiskt)
2. **Testa att endpoint fungerar**
3. **Sedan uppdatera till `nocturne-swe`**

Detta ger dig en fungerande endpoint snabbt, sedan kan du v√§xla till din anpassade modell.

---

## üîç Debugging

Om modellen inte skapas, kolla RunPod logs f√∂r:
- `[RunPod] Pulling base model...`
- `[RunPod] Creating nocturne-swe from Modelfile...`
- `[RunPod] Model nocturne-swe ready`

Om du ser fel:
- **"Model not found"** ‚Üí Basmodellen laddas fortfarande ner
- **"Modelfile not found"** ‚Üí Modelfile √§r inte kopierad till Docker image
- **"Permission denied"** ‚Üí Ollama har inte r√§ttigheter att skapa modeller

---

## ‚úÖ Checklista

- [ ] Modelfile kopierad till `gpt-relay-server/Modelfile`
- [ ] Dockerfile uppdaterad att inkludera Modelfile
- [ ] handler.py uppdaterad att skapa modellen
- [ ] Pushat till GitHub
- [ ] RunPod endpoint uppdaterad
- [ ] V√§ntat p√• build (5-10 min f√∂rsta g√•ngen)
- [ ] Testat fr√•n lokal server

