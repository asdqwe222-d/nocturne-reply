# 游빍 Testa RunPod Endpoint

## Problem: Logs 칛r tomma och kan inte testa fr친n Dashboard

Detta 칛r normalt - RunPod Hub Ollama Worker kanske inte visar logs direkt i Dashboard.

## L칬sning: Testa fr친n din server ist칛llet

### Steg 1: Verifiera .env konfiguration

칐ppna `gpt-relay-server/.env` och kontrollera:

```bash
RUNPOD_ENDPOINT_URL=https://api.runpod.ai/v2/kjuxscfof20t8h/run
RUNPOD_API_KEY=your-api-key-here
USE_RUNPOD=true
OLLAMA_MODEL=nocturne-swe
```

**VIKTIGT:** Om `nocturne-swe` inte finns i RunPod containern, 칛ndra till en modell som finns:

```bash
OLLAMA_MODEL=llama3
```

### Steg 2: Starta servern med logging

```bash
cd gpt-relay-server
node server.js
```

Du b칬r se:
```
RunPod Serverless: Enabled
   Endpoint: https://api.runpod.ai/v2/kjuxscfof20t8h/run...
```

### Steg 3: Testa att generera replies

1. 칐ppna `http://localhost:3000/test-chat.html`
2. Skriv ett meddelande
3. Klicka "Generate"

### Steg 4: Kolla server logs

I terminalen d칛r `node server.js` k칬rs, leta efter:

**Om det fungerar:**
```
[Nocturne] Calling RunPod Serverless...
[RunPod] Calling serverless endpoint: https://api.runpod.ai/v2/...
[RunPod] Model: nocturne-swe
[RunPod] Full response: {...}
[RunPod] Found output.response
[RunPod] Response received, length: XXX
```

**Om det misslyckas:**
```
[RunPod] API error: 500
[RunPod] Error details: {...}
[RunPod] Request body was: {...}
```

### Steg 5: Om modellen inte finns

Om du ser `Model not found` eller liknande:

**Option A: Anv칛nd en k칛nd modell**
1. 츿ndra i `.env`: `OLLAMA_MODEL=llama3`
2. Starta om servern
3. Testa igen

**Option B: Pulla modellen i RunPod**
- Detta kr칛ver att du har SSH-친tkomst eller kan k칬ra kommandon i containern
- RunPod Hub Ollama Worker kanske inte st칬djer detta direkt

### Steg 6: Testa med curl (alternativ)

Om servern inte fungerar, testa direkt:

```bash
curl -X POST https://api.runpod.ai/v2/kjuxscfof20t8h/run \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -d '{
    "input": {
      "model": "llama3",
      "prompt": "Hello, this is a test",
      "system": "You are helpful.",
      "stream": false
    }
  }'
```

**Ers칛tt:**
- `kjuxscfof20t8h` med ditt endpoint ID
- `YOUR_API_KEY` med din API key
- `llama3` med din modell (eller `nocturne-swe` om den finns)

---

## Vanliga Problem

### Problem: "Model not found"
**L칬sning:** 츿ndra `OLLAMA_MODEL` i `.env` till `llama3` eller `mistral`

### Problem: "Empty response"
**L칬sning:** Kolla `[RunPod] Full response:` i logs f칬r att se vad som faktiskt returneras

### Problem: "Timeout"
**L칬sning:** Request tar f칬r l친ng tid - modellen kanske 칛r f칬r stor eller containern 칛r l친ngsam

### Problem: "API error: 500"
**L칬sning:** Kolla `[RunPod] Error details:` i logs f칬r att se vad RunPod s칛ger

---

## N칛sta Steg

1. **Starta servern:** `cd gpt-relay-server && node server.js`
2. **Testa generation:** Fr친n test-chat.html eller userscript
3. **Kopiera logs:** Fr친n terminalen och skicka hit
4. **Om det inte fungerar:** Testa med `llama3` ist칛llet f칬r `nocturne-swe`

