# ‚ö° Snabb Fix f√∂r RunPod Hub Template

## Problem

Du anv√§nder RunPod Hub template som inte kan √§ndras direkt. Modellen `nocturne-swe` finns inte i Ollama's registry.

---

## ‚úÖ Snabb Fix: Anv√§nd Standardmodell

### Steg 1: √Ñndra RunPod Endpoint

1. **G√• till RunPod Dashboard ‚Üí Edit Endpoint**
2. **Hitta Environment Variables**
3. **√Ñndra `OLLAMA_MODEL_NAME`:**
   ```
   OLLAMA_MODEL_NAME=llama3
   ```
   (eller `mistral`, `llama3.2`, `phi3`)

4. **Spara Endpoint**

### Steg 2: Uppdatera Lokal `.env`

√ñppna `gpt-relay-server/.env` och √§ndra:

```bash
OLLAMA_MODEL=llama3
```

### Steg 3: Testa

```bash
cd gpt-relay-server
node server.js
```

Testa fr√•n `http://localhost:3000/test-chat.html`

**Detta fungerar direkt!** üéâ

---

## üîÑ Senare: F√∂r Din Anpassade Modell

F√∂r att anv√§nda `nocturne-swe` med RunPod Hub template beh√∂ver du:

### Option A: Bygg Egen Docker Image

1. **Bygg Docker image** med din Dockerfile som skapar modellen
2. **Push till RunPod Registry** eller Docker Hub
3. **Anv√§nd din image** i RunPod ist√§llet f√∂r template

### Option B: Anv√§nd Standardmodell + System Prompt

Du kan skicka system prompten direkt i din request ist√§llet f√∂r att ha den i Modelfile. Detta fungerar med standardmodeller!

---

## üéØ Rekommendation Nu

**Anv√§nd `llama3` f√∂r att f√• det att fungera NU.**

**Senare:** Bygg egen Docker image f√∂r din anpassade modell.

