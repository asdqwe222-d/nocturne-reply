# â³ Medan Endpoint Bygger

## Vad hÃ¤nder nu?

RunPod bygger din Docker image frÃ¥n GitHub-repo:
1. âœ… Klonar `asdqwe222-d/nocturne-reply`
2. ğŸ”„ Bygger Docker image frÃ¥n `Dockerfile`
3. ğŸ”„ Installerar Ollama
4. ğŸ”„ Installerar Python dependencies
5. ğŸ”„ Konfigurerar handler

Detta tar vanligtvis **2-5 minuter**.

---

## ğŸ“‹ Medan du vÃ¤ntar: FÃ¶rbered nÃ¤sta steg

### Steg 1: Skapa RunPod API Key

1. **Ã–ppna ny flik** i webblÃ¤saren
2. GÃ¥ till RunPod Dashboard: https://www.runpod.io
3. Klicka pÃ¥ din profil (hÃ¶gst upp till hÃ¶ger) â†’ **Settings**
4. VÃ¤lj **API Keys** i menyn
5. Klicka pÃ¥ **"Create API Key"**
6. **Kopiera nyckeln direkt** (den visas bara en gÃ¥ng!)
7. Spara den nÃ¥gonstans sÃ¤kert (t.ex. i en textfil)

**OBS:** Du behÃ¶ver denna nyckel fÃ¶r att ansluta din server till RunPod!

---

### Steg 2: FÃ¶rbered `.env` filen

Ã–ppna `gpt-relay-server/.env` i en texteditor.

LÃ¤gg till dessa rader (om de inte redan finns):

```bash
# RunPod Serverless Configuration
RUNPOD_ENDPOINT_URL=
RUNPOD_API_KEY=

# Hybrid mode (rekommenderat)
USE_RUNPOD=false
RUNPOD_FALLBACK=true
```

**Vi fyller i URL:en nÃ¤r builden Ã¤r klar!**

---

## âœ… NÃ¤r builden Ã¤r klar

### Steg 1: Kopiera Endpoint URL

1. GÃ¥ tillbaka till RunPod Dashboard â†’ **Serverless**
2. Hitta din endpoint: `nocturne-reply`
3. Klicka pÃ¥ den
4. Kopiera **Endpoint URL**
   - Format: `https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run`
   - Den finns oftast hÃ¶gst upp pÃ¥ sidan eller under "Endpoint Details"

### Steg 2: Uppdatera `.env`

Ã–ppna `gpt-relay-server/.env` och fyll i:

```bash
# RunPod Serverless Configuration
RUNPOD_ENDPOINT_URL=https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run
RUNPOD_API_KEY=your-api-key-here

# Hybrid mode (rekommenderat)
USE_RUNPOD=false
RUNPOD_FALLBACK=true
```

**ErsÃ¤tt:**
- `YOUR_ENDPOINT_ID` med det faktiska ID:et frÃ¥n URL:en
- `your-api-key-here` med API-nyckeln du kopierade tidigare

---

## ğŸ§ª Steg 3: Testa Endpoint

### Test frÃ¥n RunPod Dashboard:

1. I RunPod Dashboard â†’ Serverless â†’ din endpoint
2. Klicka pÃ¥ **"Test"** eller **"Test Endpoint"**
3. AnvÃ¤nd test-input:
```json
{
  "input": {
    "model": "nocturne-swe",
    "prompt": "Say hello in Swedish",
    "system": "You are a helpful assistant.",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "num_predict": 50
    }
  }
}
```

4. Klicka **"Run"** eller **"Send"**
5. Du bÃ¶r fÃ¥ ett svar tillbaka!

---

## ğŸš€ Steg 4: Testa frÃ¥n din server

### Starta servern:

```bash
cd C:\Users\Oliwer\Desktop\Cursor\Nocturne\gpt-relay-server
npm start
```

### Kontrollera loggarna:

Du bÃ¶r se nÃ¥got liknande:

```
ğŸŒ RunPod Serverless: ENABLED (fallback mode)
   Endpoint: https://api.runpod.io/v2/...
   Will fallback to RunPod if local Ollama fails
```

### Testa genom att generera ett svar:

1. Ã–ppna din Tampermonkey script pÃ¥ en test-sida
2. Skriv ett meddelande i chatten
3. Klicka pÃ¥ "Generate" eller tryck Ctrl+Enter
4. Om lokal Ollama inte fungerar, ska den automatiskt fallback till RunPod!

---

## ğŸ”§ Troubleshooting

### Problem: Build failed

**Kontrollera:**
- Build-loggar i RunPod Dashboard
- Verifiera att `Dockerfile` finns i repo
- Kontrollera att `handler.py` finns
- Se till att `requirements.txt` finns

**Vanliga fel:**
- "Dockerfile not found" â†’ Kontrollera att Dockerfile Ã¤r i `gpt-relay-server/` mappen
- "Handler not found" â†’ Kontrollera att handler.py finns
- "Build timeout" â†’ FÃ¶rsÃ¶k igen, kan vara temporÃ¤rt

### Problem: Endpoint URL inte synlig

**LÃ¶sning:**
- GÃ¥ till Serverless â†’ din endpoint
- Klicka pÃ¥ endpoint-namnet
- Endpoint URL finns under "Endpoint Details" eller hÃ¶gst upp

### Problem: API Key saknas

**LÃ¶sning:**
- Settings â†’ API Keys â†’ Create API Key
- Kopiera direkt (visas bara en gÃ¥ng!)

---

## ğŸ“‹ Checklista

- [ ] API Key skapad och kopierad
- [ ] `.env` fÃ¶rberedd (tomma vÃ¤rden)
- [ ] Build klar (status: "Ready")
- [ ] Endpoint URL kopierad
- [ ] `.env` uppdaterad med URL och API key
- [ ] Server startad
- [ ] Test-kÃ¶rning genomfÃ¶rd

---

## ğŸ‰ NÃ¤r allt Ã¤r klart!

Din server kommer nu automatiskt anvÃ¤nda RunPod om lokal Ollama inte fungerar!

**NÃ¤sta:** Testa genom att generera nÃ¥gra svar och se att RunPod anvÃ¤nds nÃ¤r lokal Ollama inte Ã¤r tillgÃ¤nglig.

