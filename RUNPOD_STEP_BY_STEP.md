# üéØ RunPod Setup: Steg-f√∂r-Steg Guide

## ‚úÖ Steg 1: Endpoint Configuration (Du √§r h√§r nu!)

### Endpoint Name:
- ‚úÖ `nocturne-reply` (eller √§ndra till `nocturne-ollama` om du vill)

### Endpoint Type:
- ‚úÖ **Queue** (korrekt valt - perfekt f√∂r v√•r use case)

### Worker Type:
- ‚úÖ **GPU** (korrekt valt)

### GPU Configuration:
- ‚úÖ **48 GB** (detta √§r f√∂rmodligen A40 - perfekt f√∂r st√∂rre modeller!)
- ‚úÖ **1st** (f√∂rsta prioriteten)
- ‚úÖ Kostnad: `$0.00034/s` (~$0.02/minut, ~$1.22/timme)

**OBS:** Om du vill spara pengar kan du v√§lja en mindre GPU (t.ex. RTX 3090 med 24 GB), men 48 GB √§r bra f√∂r st√∂rre modeller.

---

## üìã Steg 2: Klicka "Next" eller "Continue"

Efter att du har valt GPU-konfiguration, klicka p√• **"Next"** eller **"Continue"** f√∂r att g√• till n√§sta steg.

---

## üîß Steg 3: Container Configuration (N√§sta sida)

P√• n√§sta sida kommer du se:

### Container Start Command:
Fyll i:
```bash
python /app/handler.py
```

### Container Disk:
- √Ñndra fr√•n 5 GB till **10 GB** (klicka p√• + tills det st√•r 10 GB)
- Ollama och modeller beh√∂ver mer plats

### Expose HTTP Ports:
Fyll i:
```
11434
```

### Expose TCP Ports:
L√§mna tomt (eller fyll i `11434` om det kr√§vs)

---

## üîê Steg 4: Environment Variables

Klicka f√∂r att expandera "Environment Variables" och l√§gg till:

**Variable 1:**
- Key: `OLLAMA_MODEL`
- Value: `nocturne-swe`

**Variable 2:**
- Key: `OLLAMA_HOST`
- Value: `0.0.0.0:11434`

Klicka p√• **"+ Add Environment Variable"** f√∂r varje ny variabel.

---

## üöÄ Steg 5: Deploy

N√§r allt √§r ifyllt:
1. Klicka p√• **"Deploy Endpoint"** eller **"Create"**
2. V√§nta p√• build (2-5 minuter)
3. Du kommer se build-loggar
4. N√§r status √§r "Ready", kopiera **Endpoint URL**

---

## üìù Steg 6: Kopiera Endpoint URL och API Key

### Endpoint URL:
Format: `https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run`

### API Key:
1. G√• till RunPod Dashboard ‚Üí **Settings** ‚Üí **API Keys**
2. Klicka p√• **"Create API Key"**
3. Kopiera nyckeln direkt (den visas bara en g√•ng!)

---

## ‚öôÔ∏è Steg 7: Uppdatera `.env`

√ñppna `gpt-relay-server/.env` och l√§gg till:

```bash
# RunPod Serverless Configuration
RUNPOD_ENDPOINT_URL=https://api.runpod.io/v2/YOUR_ENDPOINT_ID/run
RUNPOD_API_KEY=your-api-key-here

# Hybrid mode (rekommenderat)
USE_RUNPOD=false
RUNPOD_FALLBACK=true
```

---

## ‚úÖ Checklista

- [x] Repository valt: `asdqwe222-d/nocturne-reply`
- [x] Endpoint Type: Queue
- [x] Worker Type: GPU
- [x] GPU: 48 GB valt
- [ ] Container Start Command: `python /app/handler.py`
- [ ] Container Disk: 10 GB
- [ ] HTTP Ports: 11434
- [ ] Environment Variables: `OLLAMA_MODEL=nocturne-swe`, `OLLAMA_HOST=0.0.0.0:11434`
- [ ] Endpoint deployad
- [ ] Endpoint URL kopierad
- [ ] API Key skapad och kopierad
- [ ] `.env` uppdaterad

---

## üí° Tips

- **GPU-val:** 48 GB (A40) √§r bra f√∂r st√∂rre modeller. Om du bara k√∂r 12B-modeller kan du v√§lja 24 GB (RTX 3090) f√∂r att spara pengar.
- **Container Disk:** 10 GB √§r minimum. Om du ska k√∂ra flera modeller, √∂verv√§g 20 GB.
- **Kostnad:** Med $0.00034/s blir det ~$0.02 per minut. En generation tar ~5 sekunder = ~$0.002 per generation.

---

**N√§sta:** Klicka "Next" och fyll i Container Configuration!

