# ğŸ’¾ Model Persistence i RunPod Serverless

## Kort svar: Det beror pÃ¥ Worker Type

### Flex Workers (Pay-per-use)
- âŒ **Ja, modellen laddas ner varje gÃ¥ng** (cold start)
- Varje anrop startar en ny container
- FÃ¶rsta anropet tar lÃ¤ngre tid (~30-60 sekunder fÃ¶r att ladda ner modellen)
- EfterfÃ¶ljande anrop i samma session Ã¤r snabba (modellen finns redan i minnet)
- Container stÃ¤ngs av efter inaktivitet

### Active Workers (Always-on)
- âœ… **Nej, modellen laddas bara en gÃ¥ng**
- Container Ã¤r alltid pÃ¥
- Modellen finns kvar i minnet mellan anrop
- Snabbare svar (ingen cold start)
- Men kostar Ã¤ven nÃ¤r den inte anvÃ¤nds

---

## ğŸš€ LÃ¶sningar fÃ¶r att minska cold starts

### LÃ¶sning 1: Pre-load modellen i Dockerfile (Rekommenderat)

Uppdatera `Dockerfile` fÃ¶r att ladda ner modellen vid build-tid:

```dockerfile
# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Pre-load model during build
RUN ollama pull nocturne-swe

# Rest of Dockerfile...
```

**FÃ¶rdelar:**
- âœ… Modellen finns redan nÃ¤r containern startar
- âœ… Snabbare fÃ¶rsta anrop (ingen nedladdning)
- âœ… Fungerar med bÃ¥de Flex och Active workers

**Nackdelar:**
- âŒ Docker image blir stÃ¶rre (~4-8 GB fÃ¶r en 12B modell)
- âŒ Build tar lÃ¤ngre tid
- âŒ Kan Ã¶ka lagringskostnader

---

### LÃ¶sning 2: AnvÃ¤nd Active Workers

**Konfigurera Active Worker:**
- I RunPod, vÃ¤lj **"Active"** istÃ¤llet fÃ¶r **"Flex"**
- Container Ã¤r alltid pÃ¥
- Modellen laddas en gÃ¥ng och finns kvar

**FÃ¶rdelar:**
- âœ… Ingen cold start efter fÃ¶rsta laddning
- âœ… Snabbare svar
- âœ… 20-30% rabatt pÃ¥ GPU-kostnad

**Nackdelar:**
- âŒ Kostar Ã¤ven nÃ¤r den inte anvÃ¤nds
- âŒ BÃ¤ttre fÃ¶r produktion Ã¤n test

---

### LÃ¶sning 3: Pre-load i handler.py (Nuvarande lÃ¶sning)

Din nuvarande `handler.py` har redan pre-load funktion:

```python
# Pre-load model on startup
def preload_model(model_name="nocturne-swe"):
    """Pre-load the model to reduce cold start time"""
    try:
        print(f"[RunPod] Pre-loading model: {model_name}")
        result = subprocess.run(['ollama', 'run', model_name, 'test'], 
                              capture_output=True, timeout=30)
        print(f"[RunPod] Model {model_name} pre-loaded")
    except Exception as e:
        print(f"[RunPod] Warning: Could not pre-load model: {e}")
```

**Detta hjÃ¤lper, men:**
- âœ… Modellen laddas nÃ¤r containern startar (inte vid varje anrop)
- âŒ FÃ¶r Flex workers: Modellen laddas fortfarande vid varje ny container-start
- âœ… FÃ¶r Active workers: Modellen laddas bara en gÃ¥ng

---

### LÃ¶sning 4: AnvÃ¤nd Volumes (Avancerat)

RunPod stÃ¶djer volumes fÃ¶r persistent storage:

1. **Skapa Volume i RunPod:**
   - Settings â†’ Volumes â†’ Create Volume
   - Storlek: 20-50 GB (fÃ¶r modeller)

2. **Mounta volume i endpoint:**
   - Volume Mount Path: `/models`
   - Spara modeller i `/models` istÃ¤llet fÃ¶r standard-plats

3. **Uppdatera handler.py:**
   ```python
   # Set Ollama model path to volume
   os.environ['OLLAMA_MODELS'] = '/models'
   ```

**FÃ¶rdelar:**
- âœ… Modeller sparas mellan container-starts
- âœ… Fungerar med Flex workers

**Nackdelar:**
- âŒ Mer komplex setup
- âŒ Extra kostnad fÃ¶r volume storage

---

## ğŸ’° Kostnad-jÃ¤mfÃ¶relse

### Scenario: 50 generationer/dag, ~5 sekunder per generation

**Flex Workers (med cold start):**
- Cold start: ~30 sekunder fÃ¶rsta gÃ¥ngen
- EfterfÃ¶ljande: ~5 sekunder
- Kostnad: ~$0.10/dag (med cold starts)

**Active Workers:**
- Inga cold starts efter fÃ¶rsta laddning
- Kostnad: ~$29/dag (alltid pÃ¥)
- Men 20-30% rabatt = ~$20-23/dag

**Rekommendation:**
- **Test/LÃ¥g anvÃ¤ndning:** Flex workers (billigare)
- **Produktion/HÃ¶g anvÃ¤ndning:** Active workers (snabbare, bÃ¤ttre UX)

---

## ğŸ¯ Rekommenderad lÃ¶sning fÃ¶r dig

### FÃ¶r nu (Test-fas):

1. **BehÃ¥ll Flex workers** (billigare)
2. **LÃ¤gg till pre-load i Dockerfile** (fÃ¶r snabbare cold starts)
3. **Acceptera cold start fÃ¶rsta gÃ¥ngen** (~30 sekunder)

### FÃ¶r produktion:

1. **Byt till Active workers** (snabbare, bÃ¤ttre UX)
2. **Pre-load i Dockerfile** (fÃ¶r snabbare fÃ¶rsta laddning)
3. **Modellen finns kvar i minnet** mellan anrop

---

## ğŸ”§ Implementera Pre-load i Dockerfile

Vill du att jag uppdaterar Dockerfile fÃ¶r att pre-loada modellen? Detta kommer:
- âœ… Minska cold start-tid frÃ¥n ~30 sekunder till ~5 sekunder
- âœ… GÃ¶ra Docker image stÃ¶rre (~4-8 GB)
- âœ… Ã–ka build-tid frÃ¥n ~2 minuter till ~5-10 minuter

---

## ğŸ“Š Cold Start Timing

**Utan pre-load:**
- Container start: ~5 sekunder
- Ollama start: ~2 sekunder
- Model download: ~20-30 sekunder (fÃ¶rsta gÃ¥ngen)
- **Total: ~30-40 sekunder fÃ¶rsta anropet**

**Med pre-load i Dockerfile:**
- Container start: ~5 sekunder
- Ollama start: ~2 sekunder
- Model load frÃ¥n disk: ~2-3 sekunder
- **Total: ~10 sekunder fÃ¶rsta anropet**

**Med Active workers:**
- Container redan startad
- Model redan laddad
- **Total: ~5 sekunder (samma som vanligt anrop)**

---

**Vill du att jag uppdaterar Dockerfile fÃ¶r att pre-loada modellen?**

