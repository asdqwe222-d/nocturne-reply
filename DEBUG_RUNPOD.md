# üîç Debug RunPod Issues

## Problem: Request Failed

Om du ser "Failed: 1" i RunPod Dashboard, g√∂r f√∂ljande:

### 1. Kolla RunPod Logs

1. **G√• till din endpoint i RunPod Dashboard**
2. **Klicka p√• "Logs" tab**
3. **Leta efter felmeddelanden**

Vanliga fel:
- `Model not found` - Modellen `nocturne-swe` finns inte
- `Connection refused` - Ollama √§r inte startad
- `Timeout` - Request tog f√∂r l√•ng tid

### 2. Testa Endpoint Direkt

I RunPod Dashboard ‚Üí din endpoint ‚Üí "Quick Start":

**Test Request:**
```json
{
  "input": {
    "model": "nocturne-swe",
    "prompt": "Hello, this is a test",
    "system": "You are a helpful assistant.",
    "stream": false,
    "options": {
      "temperature": 0.85,
      "top_p": 0.9,
      "repeat_penalty": 1.3,
      "num_ctx": 4096,
      "num_predict": 600
    }
  }
}
```

**F√∂rv√§ntat Response:**
```json
{
  "output": {
    "response": "Generated text here..."
  }
}
```

### 3. Verifiera Modell

RunPod Hub Ollama Worker beh√∂ver att modellen finns i Ollama.

**Om modellen inte finns:**
1. Modellen m√•ste vara pullad i containern
2. Eller s√• m√•ste du √§ndra till en modell som finns (t.ex. `llama3`, `mistral`)

**Testa med en k√§nd modell:**
```json
{
  "input": {
    "model": "llama3",
    "prompt": "Hello",
    "system": "You are helpful.",
    "stream": false
  }
}
```

### 4. Kolla Server Logs

I din terminal d√§r `node server.js` k√∂rs, leta efter:
- `[RunPod] Calling serverless endpoint:`
- `[RunPod] API error:`
- `[RunPod] Full response:`

Dessa logs visar exakt vad som h√§nder.

### 5. Vanliga Problem och L√∂sningar

#### Problem: "Model not found"
**L√∂sning:** 
- √Ñndra modell i `.env` till en som finns (t.ex. `llama3`)
- Eller pulla modellen i RunPod containern

#### Problem: "Empty response"
**L√∂sning:**
- Kolla RunPod logs f√∂r att se vad som faktiskt returneras
- Response-formatet kan skilja sig - kolla `DEBUG_RUNPOD.md`

#### Problem: "Timeout"
**L√∂sning:**
- √ñka timeout i `runpod-client.js` (default: 120 sekunder)
- Eller minska `num_predict` f√∂r snabbare generation

### 6. Testa Lokalt F√∂rst

Innan du anv√§nder RunPod, testa att lokal Ollama fungerar:

```bash
# Testa lokal Ollama
curl http://localhost:11434/api/generate -d '{
  "model": "nocturne-swe",
  "prompt": "Hello",
  "stream": false
}'
```

Om detta fungerar men RunPod inte g√∂r det, √§r problemet i RunPod-konfigurationen.

---

**N√§sta steg:** Kolla RunPod Logs och dela med dig av felmeddelandet!

